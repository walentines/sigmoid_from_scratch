def sigmoid(Z):
    A = 1 / (1 + np.exp(-Z))
    return A

def relu(Z):
    A = np.maximum(0, Z)
    return A

def sigmoid_backward(dA, Z):
    A = 1 / (1 + np.exp(-Z))
    dZ = dA * A * (1 - A)
    return dZ

def random_mini_batches(X, Y, mini_batch_size = 64):
    m = X.shape[1]
    mini_batches = []
    
    # Shuffle
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1, m))
    
    num_complete_minibatches = math.floor(m / mini_batch_size)
    for k in range(0, num_complete_minibatches):
        mini_batch_X = shuffled_X[:, k * mini_batch_size : (1 + k) * mini_batch_size]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (1 + k) * mini_batch_size]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Last case
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]
        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches 

mini_batches = random_mini_batches(training_set_x, training_set_y, mini_batch_size = 64)
  
def initialize_adam(parameters):
    L = len(parameters) // 2
    s = {}
    v = {}
    
    for l in range(L):
        v['dW' + str(l + 1)] = np.zeros((parameters['W' + str(l + 1)].shape[0], parameters['W' + str(l + 1)].shape[1]))
        v['db' + str(l + 1)] = np.zeros((parameters['b' + str(l + 1)].shape[0], parameters['b' + str(l + 1)].shape[1]))
        s['dW' + str(l + 1)] = np.zeros((parameters['W' + str(l + 1)].shape[0], parameters['W' + str(l + 1)].shape[1]))
        s['db' + str(l + 1)] = np.zeros((parameters['b' + str(l + 1)].shape[0], parameters['b' + str(l + 1)].shape[1]))
        
        return v, s
    
def initialize_parameters(layers_dims):
    L = len(layers_dims)
    parameters = {}
    
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2. / layers_dims[l - 1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))
    return parameters

def linear_forward(A, W, b):
    Z = np.dot(W, A) + b
    return Z

def forward_propagation(X, parameters):
    L = len(parameters) // 2
    cache = {}
    l = 0
    cache['A' + str(l)] = X
    A = cache['A' + str(l)]
    
    for l in range(L - 1):
        A_prev = A
        cache['Z' + str(l + 1)] = linear_forward(A_prev, parameters['W' + str(l + 1)], parameters['b' + str(l + 1)])
        cache['A' + str(l + 1)] = relu(cache['Z' + str(l + 1)])
        A = cache['A' + str(l + 1)]
        
    cache['Z' + str(L)] = linear_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])
    AL = sigmoid(cache['Z' + str(L)])
    cache['A' + str(L)] = AL
    return AL, cache

def forward_propagation_n(X, Y, parameters):
    L = len(parameters) // 2
    cache = {}
    l = 0
    cache['A' + str(l)] = X
    A = cache['A' + str(l)]
    
    for l in range(L - 1):
        A_prev = A
        cache['Z' + str(l + 1)] = linear_forward(A_prev, parameters['W' + str(l + 1)], parameters['b' + str(l + 1)])
        cache['A' + str(l + 1)] = relu(cache['Z' + str(l + 1)])
        A = cache['A' + str(l + 1)]
        
    cache['Z' + str(L)] = linear_forward(A, parameters['W' + str(L)], parameters['b' + str(L)])
    AL = sigmoid(cache['Z' + str(L)])
    cache['A' + str(L)] = AL
    
    m = Y.shape[1]
    
    cost = - 1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))

    cost = np.squeeze(cost)
    cost = float(cost)
    return cost, cache

def compute_cost(AL, Y):
    m = Y.shape[1]
    
    cost = - 1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))

    cost = np.squeeze(cost)
    cost = float(cost)
    return cost

def backpropagation(X, Y, parameters, cache):
    L = len(parameters) // 2
    grads = {}
    useless = {}
    m = X.shape[1]
    
    AL = cache['A' + str(L)]
    
    #dAL = - np.divide(Y, AL) + np.divide(1-Y, 1-AL)

    useless['dZ' + str(L)] = AL - Y 
    
    #useless['dZ' + str(L)] = np.multiply(dAL, np.int64(AL > 0))
    
    for l in reversed(range(0, L)):
        grads['dW' + str(l + 1)] = 1 / m * np.dot(useless['dZ' + str(l + 1)], cache['A' + str(l)].T)
        grads['db' + str(l + 1)] = 1 / m * np.sum(useless['dZ' + str(l + 1)], axis = 1, keepdims = True)
        if l != 0:
            useless['dA' + str(l)] = np.dot(parameters['W' + str(l + 1)].T, useless['dZ' + str(l + 1)])
            useless['dZ' + str(l)] = np.multiply(useless['dA' + str(l)], np.int64(cache['A' + str(l)] > 0))

    return grads

def backpropagation_2(X, Y, parameters, cache):
    L = len(parameters) // 2
    grads = {}
    useless = {}
    m = X.shape[1]
    
    AL = cache['A' + str(L)]
    
    # dAL = - np.divide(Y, AL) + np.divide(1-Y, 1-AL)

    useless['dZ' + str(L)] = AL - Y 
    
    # useless['dZ' + str(L)] = np.multiply(dAL, np.int64(AL > 0))
    
    for l in reversed(range(0, L)):
        grads['dW' + str(l + 1)] = 1 / m * np.dot(useless['dZ' + str(l + 1)], cache['A' + str(l)].T)
        grads['db' + str(l + 1)] = 1 / m * np.sum(useless['dZ' + str(l + 1)], axis = 1, keepdims = True)
        if l != 0:
            useless['dA' + str(l)] = np.dot(parameters['W' + str(l + 1)].T, useless['dZ' + str(l + 1)])
            useless['dZ' + str(l)] = np.multiply(useless['dA' + str(l)], np.int64(cache['A' + str(l)] > 0))

    return grads

def backward_propagation(X, Y, parameters, cache):
    m = X.shape[1]
    
    W2 = parameters['W2']
    A1 = cache['A1']
    A2 = cache['A2']
    
    dZ2 = A2 - Y
    
    dW2 = 1 / m * np.dot(dZ2, A1.T)
    db2 = 1 / m * np.sum(dZ2, axis = 1, keepdims = True)
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    
    dW1 = 1 / m * np.dot(dZ1, X.T)
    db1 = 1 / m * np.sum(dZ1, axis = 1, keepdims = True)
    
    grads = {'dW1': dW1,
             'db1': db1,
             'dW2': dW2,
             'db2': db2}
    return grads

def update_parameters(parameters, grads, learning_rate):
    L = len(parameters) // 2
    
    for l in range(L):
        parameters['W' + str(l + 1)] = parameters['W' + str(l + 1)] - np.dot(learning_rate, grads['dW' + str(l + 1)])
        parameters['b' + str(l + 1)] = parameters['b' + str(l + 1)] - np.dot(learning_rate, grads['db' + str(l + 1)])
    return parameters

def adam(parameters, grads, learning_rate, v, s, t, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):
    L = len(parameters) // 2
    v_corrected = {}
    s_corrected = {}
    
    for l in range(L):
        v['dW' + str(l + 1)] = beta1 * v['dW' + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]
        v['db' + str(l + 1)] = beta1 * v['db' + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]
        
        v_corrected['dW' + str(l + 1)] = v['dW' + str(l + 1)] / (1 - beta1 ** t)
        v_corrected['db' + str(l + 1)] = v['db' + str(l + 1)] / (1 - beta1 ** t)
        
        s['dW' + str(l + 1)] = beta2 * s['dW' + str(l + 1)] + (1 - beta2) * (grads['dW' + str(l + 1)] ** 2)
        s['db' + str(l + 1)] = beta2 * s['db' + str(l + 1)] + (1 - beta2) * (grads['db' + str(l + 1)] ** 2)
        
        s_corrected['dW' + str(l + 1)] = s['dW' + str(l + 1)] / (1 - beta2 ** t)
        s_corrected['db' + str(l + 1)] = s['db' + str(l + 1)] / (1 - beta2 ** t)
        
        parameters['W' + str(l + 1)] = parameters['W' + str(l + 1)] - np.dot(learning_rate, v_corrected['dW' + str(l + 1)] / (np.sqrt(s_corrected['dW' + str(l + 1)]) + epsilon))
        parameters['b' + str(l + 1)] = parameters['b' + str(l + 1)] - np.dot(learning_rate, v_corrected['db' + str(l + 1)] / (np.sqrt(s_corrected['db' + str(l + 1)]) + epsilon))
        
        return parameters, v, s
        
        def model_2(X, Y, layers_dims, learning_rate, num_iterations, print_cost = True):
    parameters = initialize_parameters(layers_dims)
    m = X.shape[1]
    num_minibatches = 64
    v, s = initialize_adam(parameters)
    t = 0
    for i in range(1, num_iterations + 1):
        epoch_cost = 0
        minibatches = random_mini_batches(X, Y)
        for minibatch in minibatches:
            (X_minibatch, Y_minibatch) = minibatch
            cost, cache = forward_propagation_n(X_minibatch, Y_minibatch, parameters)
            grads = backpropagation(X_minibatch, Y_minibatch, parameters, cache)
            t+= 1
            parameters, v, s = adam(parameters, grads, learning_rate, v, s, t)
            epoch_cost += cost
        if print_cost:
            print(epoch_cost / num_minibatches)
    return parameters

layers_dims = [8, 10, 10, 1]
parameters = model_2(training_set_x, training_set_y, layers_dims, learning_rate = 0.0001, num_iterations = 10000, print_cost = True)
